{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f431bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "print(\"Starting Improved School Data Merging Process...\")\n",
    "\n",
    "# List of specific files to process\n",
    "SPECIFIC_FILES = [\n",
    "    \"data/raw/AP_2023-24_2025-01-15_15_03_20.csv\",\n",
    "    \"data/raw/ELSI_NCES_GA_school_data.csv\",\n",
    "    \"data/raw/EOC_2023-24__GA_TST_AGGR_2025-01-14_16_19_30.csv\",\n",
    "    \"data/raw/EOG_2023-24__GA_TST_AGGR_2025-01-14_16_19_30.csv\",\n",
    "    \"data/raw/HS_Completer_Credentials_2023-24_2025-01-14_16_45_56.csv\",\n",
    "    \"data/raw/School_FESR_FY24_for_Display.csv\"\n",
    "]\n",
    "\n",
    "# Hard-coded column mappings for each file\n",
    "COLUMN_MAPPINGS = {\n",
    "    \"AP_2023-24_2025-01-15_15_03_20.csv\": {\n",
    "        \"school_name_col\": \"INSTN_NAME\"\n",
    "    },\n",
    "    \"ELSI_NCES_GA_school_data.csv\": {\n",
    "        \"school_name_col\": \"School Name\"\n",
    "    },\n",
    "    \"EOC_2023-24__GA_TST_AGGR_2025-01-14_16_19_30.csv\": {\n",
    "        \"school_name_col\": \"INSTN_NAME\"\n",
    "    },\n",
    "    \"EOG_2023-24__GA_TST_AGGR_2025-01-14_16_19_30.csv\": {\n",
    "        \"school_name_col\": \"INSTN_NAME\"\n",
    "    },\n",
    "    \"HS_Completer_Credentials_2023-24_2025-01-14_16_45_56.csv\": {\n",
    "        \"school_name_col\": \"INSTN_NAME\"\n",
    "    },\n",
    "    \"School_FESR_FY24_for_Display.csv\": {\n",
    "        \"school_name_col\": \"schoolname\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to clean and standardize school names\n",
    "def clean_school_name(name):\n",
    "    \"\"\"Standardize school names to improve matching.\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    name = str(name).lower().strip()\n",
    "    \n",
    "    # Replace common abbreviations\n",
    "    replacements = {\n",
    "        'elem': 'elementary',\n",
    "        'sch': 'school',\n",
    "        'schl': 'school',\n",
    "        'hs': 'high school',\n",
    "        'h.s.': 'high school',\n",
    "        'ms': 'middle school',\n",
    "        'm.s.': 'middle school',\n",
    "        'es': 'elementary school',\n",
    "        'e.s.': 'elementary school',\n",
    "        'acad': 'academy',\n",
    "        'tech': 'technical',\n",
    "        'alt': 'alternative',\n",
    "        'ctr': 'center',\n",
    "        'intl': 'international',\n",
    "        'jr.': 'junior',\n",
    "        'sr.': 'senior',\n",
    "        'pk': 'pre-k',\n",
    "        'prek': 'pre-k'\n",
    "    }\n",
    "    \n",
    "    for abbr, full in replacements.items():\n",
    "        # Replace the abbreviation if it's a standalone word\n",
    "        name = name.replace(f\" {abbr} \", f\" {full} \")\n",
    "        # Also check if it's at the end of the name\n",
    "        if name.endswith(f\" {abbr}\"):\n",
    "            name = name[:-len(abbr)] + full\n",
    "    \n",
    "    # Remove special characters and extra spaces\n",
    "    name = ''.join(e for e in name if e.isalnum() or e.isspace())\n",
    "    name = ' '.join(name.split())\n",
    "    \n",
    "    return name\n",
    "\n",
    "# Function to find the best match for a school name in another dataset\n",
    "def find_best_match(name, name_list, threshold=85):\n",
    "    \"\"\"\n",
    "    Find the best matching school name in name_list for the given name.\n",
    "    Returns the best match and the match score.\n",
    "    \"\"\"\n",
    "    if pd.isna(name) or name == \"\":\n",
    "        return None, 0\n",
    "    \n",
    "    # Get the top 3 matches\n",
    "    matches = process.extract(name, name_list, scorer=fuzz.token_sort_ratio, limit=3)\n",
    "    \n",
    "    # Filter matches by threshold\n",
    "    valid_matches = [m for m in matches if m[1] >= threshold]\n",
    "    \n",
    "    if valid_matches:\n",
    "        # Return the best match and its score\n",
    "        return valid_matches[0]\n",
    "    else:\n",
    "        return None, 0\n",
    "\n",
    "# Load the filtered NCES schools data which has the schools we want to keep\n",
    "try:\n",
    "    filtered_file = \"data/cleaned/filtered_nces_schools.csv\"\n",
    "    print(f\"Loading filtered schools data from {filtered_file}...\")\n",
    "    \n",
    "    filtered_schools = pd.read_csv(filtered_file)\n",
    "    print(f\"Loaded filtered schools data with {len(filtered_schools)} schools\")\n",
    "    \n",
    "    # Try to determine the school name column and zip code column\n",
    "    school_name_col = None\n",
    "    possible_name_cols = ['School Name', 'SCHNAM', 'school_name', 'NAME', 'name', 'School']\n",
    "    \n",
    "    for col in possible_name_cols:\n",
    "        if col in filtered_schools.columns:\n",
    "            school_name_col = col\n",
    "            print(f\"Found school name column: {school_name_col}\")\n",
    "            break\n",
    "    \n",
    "    if not school_name_col:\n",
    "        # Show the first few columns to help the user identify the school name column\n",
    "        print(\"\\nAvailable columns in filtered schools data:\")\n",
    "        for i, col in enumerate(filtered_schools.columns[:10]):\n",
    "            print(f\"{i+1}. {col}\")\n",
    "        \n",
    "        school_name_col = input(\"Enter the name of the school name column: \")\n",
    "    \n",
    "    # Find the zip code column\n",
    "    zip_col = None\n",
    "    possible_zip_cols = ['Zip Code', 'ZIP', 'zip', 'zipcode', 'ZIP_CODE', 'SCHZIP', 'Location ZIP [Public School] 2023-24']\n",
    "    \n",
    "    for col in possible_zip_cols:\n",
    "        if col in filtered_schools.columns:\n",
    "            zip_col = col\n",
    "            print(f\"Found zip code column: {zip_col}\")\n",
    "            break\n",
    "    \n",
    "    if not zip_col:\n",
    "        # Look for column names containing 'zip'\n",
    "        for col in filtered_schools.columns:\n",
    "            if 'zip' in col.lower():\n",
    "                zip_col = col\n",
    "                print(f\"Found likely zip code column: {zip_col}\")\n",
    "                break\n",
    "    \n",
    "    if not zip_col:\n",
    "        # Show columns to help user identify zip code column\n",
    "        print(\"\\nAvailable columns in filtered schools data:\")\n",
    "        for i, col in enumerate(filtered_schools.columns[:10]):\n",
    "            print(f\"{i+1}. {col}\")\n",
    "        \n",
    "        zip_col = input(\"Enter the name of the zip code column: \")\n",
    "    \n",
    "    # Create a clean version of school names for matching\n",
    "    filtered_schools['clean_name'] = filtered_schools[school_name_col].apply(clean_school_name)\n",
    "    \n",
    "    # Create a unique ID for each school based on name and zip\n",
    "    filtered_schools['school_id'] = filtered_schools.apply(\n",
    "        lambda row: f\"{row['clean_name']}_{str(row[zip_col])}\", axis=1\n",
    "    )\n",
    "    \n",
    "    # Keep only essential columns for the reference dataset\n",
    "    filtered_reference = filtered_schools[[school_name_col, zip_col, 'clean_name', 'school_id']].copy()\n",
    "    print(f\"Reference data ready with {len(filtered_reference)} schools\")\n",
    "    \n",
    "    # This will be our base dataframe that we'll merge additional data into\n",
    "    # Start with the filtered schools data as the foundation\n",
    "    merged_data = filtered_schools.copy()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading filtered schools data: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Process each data file and merge into the base dataframe\n",
    "for file_path in SPECIFIC_FILES:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nProcessing {file_name}...\")\n",
    "        \n",
    "        # Skip the NCES data file since we already used it as our base\n",
    "        if file_name == \"data/raw/ELSI_NCES_GA_school_data.csv\":\n",
    "            print(f\"Skipping {file_name} as it's already used as the base dataset\")\n",
    "            continue\n",
    "        \n",
    "        # Load the data\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(f\"Loaded data with {len(data)} rows and {len(data.columns)} columns\")\n",
    "        \n",
    "        # Get the column mappings for this file\n",
    "        file_mapping = COLUMN_MAPPINGS.get(file_name, {})\n",
    "        data_name_col = file_mapping.get(\"school_name_col\")\n",
    "        \n",
    "        # If we don't have a predefined mapping, try to detect the school name column\n",
    "        if not data_name_col or data_name_col not in data.columns:\n",
    "            possible_name_cols = ['School Name', 'SCHNAM', 'school_name', 'NAME', 'name', 'School']\n",
    "            for col in possible_name_cols:\n",
    "                if col in data.columns:\n",
    "                    data_name_col = col\n",
    "                    print(f\"Found school name column in data: {data_name_col}\")\n",
    "                    break\n",
    "        \n",
    "        if not data_name_col or data_name_col not in data.columns:\n",
    "            print(f\"Could not find school name column in {file_name}. Available columns:\")\n",
    "            for i, col in enumerate(data.columns[:10]):\n",
    "                print(f\"{i+1}. {col}\")\n",
    "            \n",
    "            data_name_col = input(f\"Please enter the name of the school name column for {file_name}: \")\n",
    "        \n",
    "        # Clean the school names for matching\n",
    "        data['clean_name'] = data[data_name_col].apply(clean_school_name)\n",
    "        \n",
    "        # Add a source file column to track where data came from\n",
    "        data['source_file'] = file_name\n",
    "        \n",
    "        # Create a mapping dictionary to store matches\n",
    "        matches = {}\n",
    "        \n",
    "        # Get unique list of clean school names in this data file\n",
    "        data_schools = data['clean_name'].dropna().unique().tolist()\n",
    "        \n",
    "        # Track matches for reporting\n",
    "        matches_found = 0\n",
    "        total_schools = len(filtered_reference)\n",
    "        \n",
    "        # For each school in the filtered reference dataset, find the best match in this data file\n",
    "        print(f\"Matching schools from {file_name} with filtered schools...\")\n",
    "        \n",
    "        for i, row in filtered_reference.iterrows():\n",
    "            school_name = row['clean_name']\n",
    "            original_name = row[school_name_col]\n",
    "            zip_code = row[zip_col]\n",
    "            school_id = row['school_id']\n",
    "            \n",
    "            # Find the best match\n",
    "            best_match, score = find_best_match(school_name, data_schools)\n",
    "            \n",
    "            if best_match and score >= 85:\n",
    "                # Found a good match - store the mapping\n",
    "                matches[school_id] = best_match\n",
    "                matches_found += 1\n",
    "        \n",
    "        print(f\"Found {matches_found} matches out of {total_schools} filtered schools\")\n",
    "        \n",
    "        if matches_found == 0:\n",
    "            print(f\"No matches found for {file_name}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Add the school_id to the data file for merging\n",
    "        data['school_id'] = np.nan\n",
    "        \n",
    "        # Use the matches dictionary to assign school_ids to matching rows\n",
    "        for school_id, match_name in matches.items():\n",
    "            # Find all rows with this clean_name\n",
    "            matching_rows = data['clean_name'] == match_name\n",
    "            # Assign the school_id to these rows\n",
    "            data.loc[matching_rows, 'school_id'] = school_id\n",
    "        \n",
    "        # Filter to keep only rows with a school_id (matched schools)\n",
    "        matched_data = data[data['school_id'].notna()].copy()\n",
    "        \n",
    "        # For datasets with multiple rows per school (e.g., different test subjects)\n",
    "        # We need to determine how to handle them\n",
    "        school_counts = matched_data['school_id'].value_counts()\n",
    "        multiple_rows = school_counts[school_counts > 1].index.tolist()\n",
    "        \n",
    "        if multiple_rows:\n",
    "            print(f\"Found {len(multiple_rows)} schools with multiple rows in {file_name}\")\n",
    "            print(\"For these schools, we'll create separate columns for each unique value\")\n",
    "            \n",
    "            # Get columns that might vary across rows for the same school\n",
    "            # Exclude certain columns we know are consistent\n",
    "            exclude_cols = ['clean_name', 'school_id', 'source_file', data_name_col]\n",
    "            potential_varying_cols = [col for col in matched_data.columns if col not in exclude_cols]\n",
    "            \n",
    "            # For each school with multiple rows\n",
    "            for school_id in multiple_rows:\n",
    "                # Get all rows for this school\n",
    "                school_rows = matched_data[matched_data['school_id'] == school_id]\n",
    "                \n",
    "                # Look for columns that have different values across rows\n",
    "                for col in potential_varying_cols:\n",
    "                    unique_values = school_rows[col].dropna().unique()\n",
    "                    \n",
    "                    # If there are multiple unique values, create separate columns\n",
    "                    if len(unique_values) > 1:\n",
    "                        # Find a distinguishing column to use as a suffix\n",
    "                        # Try common differentiators like subject, grade, etc.\n",
    "                        differentiator_cols = ['Subject', 'Grade', 'SUBGROUP_NAME', 'TEST_CMPNT_TYP_NM', 'ACDMC_LVL']\n",
    "                        \n",
    "                        differentiator = None\n",
    "                        for diff_col in differentiator_cols:\n",
    "                            if diff_col in school_rows.columns and school_rows[diff_col].nunique() > 1:\n",
    "                                differentiator = diff_col\n",
    "                                break\n",
    "                        \n",
    "                        if differentiator:\n",
    "                            # Create new columns with the differentiator value as a suffix\n",
    "                            for _, row in school_rows.iterrows():\n",
    "                                diff_value = row[differentiator]\n",
    "                                if pd.notna(diff_value) and pd.notna(row[col]):\n",
    "                                    # Create a new column name with the differentiator\n",
    "                                    new_col = f\"{col}_{diff_value}\"\n",
    "                                    # Clean up the column name\n",
    "                                    new_col = new_col.replace(\" \", \"_\").replace(\".\", \"\").replace(\"/\", \"_\")\n",
    "                                    # Add this value to the first row for this school\n",
    "                                    matched_data.loc[matched_data['school_id'] == school_id, new_col] = row[col]\n",
    "            \n",
    "            # Now collapse to one row per school by grouping on school_id\n",
    "            # For any remaining duplicate values, we'll use the first one\n",
    "            matched_data = matched_data.groupby('school_id').first().reset_index()\n",
    "            print(f\"After collapsing to one row per school: {len(matched_data)} rows\")\n",
    "        \n",
    "        # Drop the clean_name column as it was just for matching\n",
    "        if 'clean_name' in matched_data.columns:\n",
    "            matched_data = matched_data.drop('clean_name', axis=1)\n",
    "        \n",
    "        # Prepare column names for merge - prefix columns with source file to avoid conflicts\n",
    "        # Exclude certain columns from prefix (common joining columns)\n",
    "        no_prefix_cols = ['school_id', school_name_col, zip_col]\n",
    "        \n",
    "        # Create a prefix for this data source\n",
    "        file_prefix = file_name.split('_')[0].lower() + \"_\"\n",
    "        \n",
    "        # Create a dictionary to rename columns\n",
    "        rename_dict = {}\n",
    "        for col in matched_data.columns:\n",
    "            if col not in no_prefix_cols:\n",
    "                # Check if column already exists in merged_data\n",
    "                if col in merged_data.columns:\n",
    "                    rename_dict[col] = file_prefix + col\n",
    "        \n",
    "        # Rename columns to avoid conflicts\n",
    "        matched_data = matched_data.rename(columns=rename_dict)\n",
    "        \n",
    "        # Check for any remaining column conflicts\n",
    "        conflicts = [col for col in matched_data.columns if col in merged_data.columns and col != 'school_id']\n",
    "        if conflicts:\n",
    "            print(f\"Warning: Column conflicts found: {conflicts}\")\n",
    "            print(\"These columns will be overwritten in the merged dataset\")\n",
    "        \n",
    "        # Merge with the main dataframe\n",
    "        print(f\"Merging {len(matched_data)} rows from {file_name} into main dataset\")\n",
    "        merged_data = pd.merge(\n",
    "            merged_data, \n",
    "            matched_data,\n",
    "            on='school_id',\n",
    "            how='left',\n",
    "            suffixes=('', f'_{file_prefix}')\n",
    "        )\n",
    "        print(f\"Merged dataset now has {len(merged_data)} rows and {len(merged_data.columns)} columns\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        print(f\"Continuing with next file...\")\n",
    "\n",
    "# Drop the temporary columns used for merging\n",
    "if 'clean_name' in merged_data.columns:\n",
    "    merged_data = merged_data.drop('clean_name', axis=1)\n",
    "\n",
    "# Save the properly merged dataset\n",
    "try:\n",
    "    output_filename = \"data/aggregated/merged_school_data.csv\"\n",
    "    merged_data.to_csv(output_filename, index=False)\n",
    "    print(f\"\\nSaved properly merged dataset with {len(merged_data)} rows and {len(merged_data.columns)} columns to {output_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving merged dataset: {e}\")\n",
    "\n",
    "print(\"\\nProcessing complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
