{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce35ea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import re\n",
    "\n",
    "print(\"Starting School Quality Metrics Evaluation...\")\n",
    "\n",
    "# Configuration - metric categories and their weights\n",
    "METRIC_WEIGHTS = {\n",
    "    'academic': 0.35,     # Test scores, proficiency rates\n",
    "    'advanced': 0.15,     # AP scores, advanced courses\n",
    "    'resources': 0.15,    # Funding, student-teacher ratio\n",
    "    'equity': 0.15,       # Achievement gaps, free/reduced lunch performance\n",
    "    'environment': 0.10,  # School climate, student engagement\n",
    "    'outcomes': 0.10      # Graduation rates, college readiness\n",
    "}\n",
    "\n",
    "# Define patterns to identify columns related to each metric category\n",
    "METRIC_PATTERNS = {\n",
    "    'academic': [\n",
    "        'PROFICIENT', 'DISTINGUISHED', 'test', 'score', 'assessment', 'achievement',\n",
    "        'reading', 'math', 'science', 'EOG', 'EOC', 'LABEL_LVL'\n",
    "    ],\n",
    "    'advanced': [\n",
    "        'AP', 'advanced placement', 'honors', 'gifted', 'accelerated',\n",
    "        'dual enrollment', 'international baccalaureate', 'NUMBER_TESTS_3_OR_HIGHER'\n",
    "    ],\n",
    "    'resources': [\n",
    "        'PPE', 'expenditure', 'spend', 'ratio', 'Pupil/Teacher', 'funding',\n",
    "        'Federal_Amt', 'State_Local_Amt', 'allocation'\n",
    "    ],\n",
    "    'equity': [\n",
    "        'gap', 'subgroup', 'Free Lunch', 'Reduced-price Lunch', 'Direct Certification',\n",
    "        'economically disadvantaged', 'equity'\n",
    "    ],\n",
    "    'environment': [\n",
    "        'climate', 'attendance', 'absent', 'discipline', 'suspension',\n",
    "        'safety', 'engagement', 'extracurricular'\n",
    "    ],\n",
    "    'outcomes': [\n",
    "        'graduation', 'dropout', 'college', 'career', 'readiness', 'completer',\n",
    "        'credential', 'FESR', 'COMPLETER_TYPE'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define which metrics are positive (higher is better) vs negative (lower is better)\n",
    "NEGATIVE_METRICS = [\n",
    "    'absent', 'dropout', 'suspension', 'discipline', 'gap'\n",
    "]\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load the school data from CSV.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Loaded data with {len(df)} rows and {len(df.columns)} columns\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "def identify_key_columns(df):\n",
    "    \"\"\"Identify the school name, zip code, and key metric columns.\"\"\"\n",
    "    # Find school name column\n",
    "    school_name_col = None\n",
    "    possible_name_cols = ['School Name', 'SCHNAM', 'school_name', 'NAME', 'name', 'School', \n",
    "                          'INSTN_NAME', 'School Name [Public School] 2023-24']\n",
    "    \n",
    "    for col in possible_name_cols:\n",
    "        if col in df.columns:\n",
    "            school_name_col = col\n",
    "            print(f\"Found school name column: {school_name_col}\")\n",
    "            break\n",
    "    \n",
    "    if not school_name_col:\n",
    "        print(\"Could not identify school name column. Using first column as default.\")\n",
    "        school_name_col = df.columns[0]\n",
    "    \n",
    "    # Find zip code column\n",
    "    zip_col = None\n",
    "    possible_zip_cols = ['Zip Code', 'ZIP', 'zip', 'zipcode', 'ZIP_CODE', 'SCHZIP',\n",
    "                         'Location ZIP [Public School] 2023-24', 'zip_col']\n",
    "    \n",
    "    for col in possible_zip_cols:\n",
    "        if col in df.columns:\n",
    "            zip_col = col\n",
    "            print(f\"Found zip code column: {zip_col}\")\n",
    "            break\n",
    "    \n",
    "    if not zip_col:\n",
    "        # Look for column names containing 'zip'\n",
    "        for col in df.columns:\n",
    "            if 'zip' in col.lower():\n",
    "                zip_col = col\n",
    "                print(f\"Found likely zip code column: {zip_col}\")\n",
    "                break\n",
    "    \n",
    "    if not zip_col:\n",
    "        print(\"Could not identify zip code column.\")\n",
    "        zip_col = None\n",
    "    \n",
    "    # Identify metric columns by category\n",
    "    metric_columns = {category: [] for category in METRIC_WEIGHTS.keys()}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col == school_name_col or col == zip_col:\n",
    "            continue\n",
    "            \n",
    "        # Check if column contains numeric data (potential metric)\n",
    "        if df[col].dtype in ['int64', 'float64'] or pd.to_numeric(df[col], errors='coerce').notna().any():\n",
    "            # Determine which category this metric belongs to\n",
    "            for category, patterns in METRIC_PATTERNS.items():\n",
    "                if any(pattern.lower() in col.lower() for pattern in patterns):\n",
    "                    metric_columns[category].append(col)\n",
    "                    break\n",
    "    \n",
    "    # Print summary of identified metric columns\n",
    "    for category, columns in metric_columns.items():\n",
    "        if columns:\n",
    "            print(f\"Found {len(columns)} columns for {category} metrics\")\n",
    "    \n",
    "    return school_name_col, zip_col, metric_columns\n",
    "\n",
    "def is_positive_metric(col_name):\n",
    "    \"\"\"Determine if a metric is positive (higher is better) or negative (lower is better).\"\"\"\n",
    "    # Default assumption: higher values are better\n",
    "    return not any(neg_pattern.lower() in col_name.lower() for neg_pattern in NEGATIVE_METRICS)\n",
    "\n",
    "def calculate_category_scores(df, metric_columns):\n",
    "    \"\"\"Calculate normalized scores for each metric category.\"\"\"\n",
    "    category_scores = {}\n",
    "    df_scores = df.copy()\n",
    "    \n",
    "    # Scaler for normalizing metrics to 0-1 range\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    for category, columns in metric_columns.items():\n",
    "        if not columns:\n",
    "            print(f\"No columns found for {category} category. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        category_df = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # Process each metric column\n",
    "        for col in columns:\n",
    "            # Convert to numeric, handling non-numeric values\n",
    "            series = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Skip if too many missing values\n",
    "            if series.isna().sum() > 0.5 * len(series):\n",
    "                print(f\"Skipping {col} - too many missing values\")\n",
    "                continue\n",
    "                \n",
    "            # Fill remaining missing values with median\n",
    "            series = series.fillna(series.median())\n",
    "            \n",
    "            # Normalize the values to 0-1 scale\n",
    "            try:\n",
    "                normalized = scaler.fit_transform(series.values.reshape(-1, 1)).flatten()\n",
    "                \n",
    "                # Invert if this is a negative metric (lower is better)\n",
    "                if not is_positive_metric(col):\n",
    "                    normalized = 1 - normalized\n",
    "                    \n",
    "                category_df[col] = normalized\n",
    "            except Exception as e:\n",
    "                print(f\"Error normalizing {col}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate the mean score for this category (if we have metrics)\n",
    "        if not category_df.empty:\n",
    "            category_scores[category] = category_df.mean(axis=1)\n",
    "            df_scores[f'{category}_score'] = category_df.mean(axis=1)\n",
    "            \n",
    "            # Store which metrics contributed to this category\n",
    "            metric_count = len(category_df.columns)\n",
    "            df_scores[f'{category}_metrics_used'] = metric_count\n",
    "            df_scores[f'{category}_metrics'] = ', '.join(category_df.columns[:5]) + \\\n",
    "                                              (f' and {metric_count-5} more' if metric_count > 5 else '')\n",
    "        else:\n",
    "            # No valid metrics for this category\n",
    "            category_scores[category] = pd.Series(np.nan, index=df.index)\n",
    "            df_scores[f'{category}_score'] = np.nan\n",
    "            df_scores[f'{category}_metrics_used'] = 0\n",
    "            df_scores[f'{category}_metrics'] = 'None'\n",
    "    \n",
    "    return category_scores, df_scores\n",
    "\n",
    "def calculate_overall_scores(df_scores, category_scores):\n",
    "    \"\"\"Calculate weighted overall quality scores.\"\"\"\n",
    "    # Calculate weighted sum of category scores\n",
    "    overall_scores = pd.Series(0.0, index=df_scores.index)\n",
    "    weights_applied = pd.Series(0.0, index=df_scores.index)\n",
    "    \n",
    "    for category, weight in METRIC_WEIGHTS.items():\n",
    "        if category in category_scores:\n",
    "            # Only apply weight if we have data for this category\n",
    "            mask = ~category_scores[category].isna()\n",
    "            overall_scores.loc[mask] += category_scores[category].loc[mask] * weight\n",
    "            weights_applied.loc[mask] += weight\n",
    "    \n",
    "    # Adjust for missing categories\n",
    "    mask = weights_applied > 0\n",
    "    overall_scores.loc[mask] = overall_scores.loc[mask] / weights_applied.loc[mask]\n",
    "    \n",
    "    # Convert to 0-100 scale\n",
    "    overall_scores = overall_scores * 100\n",
    "    \n",
    "    # Assign letter grades based on the distribution of scores\n",
    "    # Find the min and max scores\n",
    "    min_score = overall_scores.min()\n",
    "    max_score = overall_scores.max()\n",
    "    score_range = max_score - min_score\n",
    "    \n",
    "    # Create grade boundaries based on the distribution\n",
    "    grade_boundaries = {\n",
    "        'A': min_score + (score_range * 0.8),  # Top 20%\n",
    "        'B': min_score + (score_range * 0.6),  # 60-80%\n",
    "        'C': min_score + (score_range * 0.4),  # 40-60%\n",
    "        'D': min_score + (score_range * 0.2),  # 20-40%\n",
    "        'F': min_score                         # Bottom 20%\n",
    "    }\n",
    "    \n",
    "    # Assign letter grades based on these boundaries\n",
    "    letter_grades = pd.Series(index=overall_scores.index)\n",
    "    letter_grades.loc[overall_scores >= grade_boundaries['A']] = 'A'\n",
    "    letter_grades.loc[(overall_scores >= grade_boundaries['B']) & (overall_scores < grade_boundaries['A'])] = 'B'\n",
    "    letter_grades.loc[(overall_scores >= grade_boundaries['C']) & (overall_scores < grade_boundaries['B'])] = 'C'\n",
    "    letter_grades.loc[(overall_scores >= grade_boundaries['D']) & (overall_scores < grade_boundaries['C'])] = 'D'\n",
    "    letter_grades.loc[overall_scores < grade_boundaries['D']] = 'F'\n",
    "    \n",
    "    print(f\"\\nGrade Distribution Boundaries:\")\n",
    "    print(f\"A: >= {grade_boundaries['A']:.1f}\")\n",
    "    print(f\"B: {grade_boundaries['B']:.1f} - {grade_boundaries['A']:.1f}\")\n",
    "    print(f\"C: {grade_boundaries['C']:.1f} - {grade_boundaries['B']:.1f}\")\n",
    "    print(f\"D: {grade_boundaries['D']:.1f} - {grade_boundaries['C']:.1f}\")\n",
    "    print(f\"F: < {grade_boundaries['D']:.1f}\")\n",
    "    \n",
    "    return overall_scores, letter_grades\n",
    "\n",
    "def generate_quality_report(df, school_name_col, zip_col, overall_scores, letter_grades, df_scores):\n",
    "    \"\"\"Generate the final quality report with schools, ratings, and metrics used.\"\"\"\n",
    "    # Create the base DataFrame for the report\n",
    "    report = pd.DataFrame({\n",
    "        'School Name': df[school_name_col] if school_name_col else \"Unknown\",\n",
    "        'Quality Score': overall_scores.round(1),\n",
    "        'Letter Grade': letter_grades\n",
    "    })\n",
    "    \n",
    "    # Add ZIP code if found\n",
    "    if zip_col:\n",
    "        report['ZIP Code'] = df[zip_col]\n",
    "    \n",
    "    # Add category scores (excluding Advanced Score as requested)\n",
    "    for category in METRIC_WEIGHTS.keys():\n",
    "        if category == 'advanced':  # Skip the Advanced Score as requested\n",
    "            continue\n",
    "            \n",
    "        score_col = f'{category}_score'\n",
    "        if score_col in df_scores.columns:\n",
    "            report[f'{category.capitalize()} Score'] = df_scores[score_col].round(2) * 100\n",
    "    \n",
    "    # Calculate strengths and weaknesses\n",
    "    category_columns = [f'{category.capitalize()} Score' for category in METRIC_WEIGHTS.keys() \n",
    "                        if f'{category}_score' in df_scores.columns and category != 'advanced']\n",
    "    \n",
    "    if category_columns:\n",
    "        # Find top 2 categories for each school\n",
    "        report['Strengths'] = report[category_columns].apply(\n",
    "            lambda x: ', '.join([col.split()[0] for col in category_columns \n",
    "                                if pd.notna(x[col]) and x[col] >= x[category_columns].median()]), \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Find bottom 2 categories for each school\n",
    "        report['Areas for Improvement'] = report[category_columns].apply(\n",
    "            lambda x: ', '.join([col.split()[0] for col in category_columns \n",
    "                                if pd.notna(x[col]) and x[col] < x[category_columns].median()]), \n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    return report\n",
    "\n",
    "def main():\n",
    "    # Load the merged school data\n",
    "    data_file = \"data/aggregated/merged_school_data.csv\"\n",
    "    df = load_data(data_file)\n",
    "    \n",
    "    # Identify key columns\n",
    "    school_name_col, zip_col, metric_columns = identify_key_columns(df)\n",
    "    \n",
    "    # Calculate category scores\n",
    "    category_scores, df_scores = calculate_category_scores(df, metric_columns)\n",
    "    \n",
    "    # Calculate overall quality scores\n",
    "    overall_scores, letter_grades = calculate_overall_scores(df_scores, category_scores)\n",
    "    \n",
    "    # Generate the final report\n",
    "    quality_report = generate_quality_report(df, school_name_col, zip_col, \n",
    "                                            overall_scores, letter_grades, df_scores)\n",
    "    \n",
    "    # Save the report\n",
    "    output_file = \"data/aggregated/school_quality_ratings.csv\"\n",
    "    quality_report.to_csv(output_file, index=False)\n",
    "    print(f\"Saved quality ratings for {len(quality_report)} schools to {output_file}\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    grade_counts = quality_report['Letter Grade'].value_counts()\n",
    "    print(\"\\nGrade Distribution:\")\n",
    "    for grade in ['A', 'B', 'C', 'D', 'F']:\n",
    "        if grade in grade_counts:\n",
    "            print(f\"{grade}: {grade_counts[grade]} schools ({grade_counts[grade]/len(quality_report)*100:.1f}%)\")\n",
    "    \n",
    "    # Display top and bottom schools\n",
    "    print(\"\\nTop 5 Schools:\")\n",
    "    top_schools = quality_report.nlargest(5, 'Quality Score')\n",
    "    for idx, row in top_schools.iterrows():\n",
    "        print(f\"{row['School Name']}: {row['Quality Score']:.1f} ({row['Letter Grade']})\")\n",
    "    \n",
    "    print(\"\\nBottom 5 Schools:\")\n",
    "    bottom_schools = quality_report.nsmallest(5, 'Quality Score')\n",
    "    for idx, row in bottom_schools.iterrows():\n",
    "        print(f\"{row['School Name']}: {row['Quality Score']:.1f} ({row['Letter Grade']})\")\n",
    "    \n",
    "    print(\"\\nEvaluation complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
